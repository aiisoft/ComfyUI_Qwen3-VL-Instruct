# å¯¼å…¥å¿…è¦çš„Pythonæ¨¡å—
import os  # ç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œ
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import folder_paths  # ComfyUIçš„è·¯å¾„ç®¡ç†æ¨¡å—
from torchvision.transforms import ToPILImage  # ç”¨äºå°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
from transformers import (
    Qwen3VLForConditionalGeneration,  # Qwen3-VLæ¨¡å‹ç±»
    AutoProcessor,  # è‡ªåŠ¨åŠ è½½æ¨¡å‹å¤„ç†å™¨
    BitsAndBytesConfig,  # é‡åŒ–é…ç½®ç±»
)
import comfy.model_management  # ComfyUIçš„æ¨¡å‹ç®¡ç†æ¨¡å—
from qwen_vl_utils import process_vision_info  # Qwen-VLå·¥å…·å‡½æ•°ï¼Œå¤„ç†è§†è§‰ä¿¡æ¯
from pathlib import Path  # ç”¨äºè·¯å¾„æ“ä½œçš„é«˜çº§åº“


# Qwen3-VLè§†è§‰é—®ç­”èŠ‚ç‚¹ç±»
class Qwen3_VQA:
    def __init__(self):
        """åˆå§‹åŒ–Qwen3-VLèŠ‚ç‚¹"""
        self.model_checkpoint = None  # æ¨¡å‹æ–‡ä»¶è·¯å¾„
        self.processor = None  # æ¨¡å‹å¤„ç†å™¨ï¼ˆç”¨äºæ–‡æœ¬å’Œå›¾åƒçš„é¢„å¤„ç†ï¼‰
        self.model = None  # åŠ è½½çš„æ¨¡å‹å®ä¾‹
        self.device = comfy.model_management.get_torch_device()  # è·å–å¯ç”¨çš„è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒbfloat16ç²¾åº¦ï¼ˆéœ€è¦NVIDIA Ampereæ¶æ„æˆ–æ›´æ–°çš„GPUï¼‰
        self.bf16_support = (
            torch.cuda.is_available()  # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
            and torch.cuda.get_device_capability(self.device)[0] >= 8  # æ£€æŸ¥GPUæ¶æ„ç‰ˆæœ¬
        )
        self.current_model_id = None  # è·Ÿè¸ªå½“å‰ä½¿ç”¨çš„æ¨¡å‹ID
        self.current_quantization = None  # è·Ÿè¸ªå½“å‰çš„é‡åŒ–è®¾ç½®

    @classmethod
    def INPUT_TYPES(s):
        """å®šä¹‰èŠ‚ç‚¹çš„è¾“å…¥å‚æ•°ç±»å‹å’Œé»˜è®¤å€¼"""
        return {
            "required": {  # å¿…éœ€è¾“å…¥å‚æ•°
                "text": ("STRING", {"default": "", "multiline": True}),  # ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
                "model": (  # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰èœå•
                    [
                        "Qwen3-VL-4B-Instruct-FP8",
                        "Qwen3-VL-4B-Thinking-FP8",
                        "Qwen3-VL-8B-Instruct-FP8",
                        "Qwen3-VL-8B-Thinking-FP8",
                        "Qwen3-VL-4B-Instruct",
                        "Qwen3-VL-4B-Thinking",
                        "Qwen3-VL-8B-Instruct",
                        "Qwen3-VL-8B-Thinking",
                        "Huihui-Qwen3-VL-4B-Instruct-abliterated",
                        "Huihui-Qwen3-VL-8B-Instruct-abliterated",
                    ],
                    {"default": "Qwen3-VL-4B-Instruct-FP8"},  # é»˜è®¤æ¨¡å‹
                ),
                "quantization": (  # é‡åŒ–è®¾ç½®ï¼ˆå‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼‰
                    ["none", "4bit", "8bit"],  # æ— é‡åŒ–ã€4ä½é‡åŒ–ã€8ä½é‡åŒ–
                    {"default": "none"},  # é»˜è®¤æ— é‡åŒ–
                ),
                "keep_model_loaded": ("BOOLEAN", {"default": False}),  # æ˜¯å¦ä¿æŒæ¨¡å‹åŠ è½½åœ¨å†…å­˜ä¸­
                "temperature": (  # ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼ˆ0-1ï¼Œå€¼è¶Šå¤§è¶Šéšæœºï¼‰
                    "FLOAT",
                    {"default": 0.7, "min": 0, "max": 1, "step": 0.1},
                ),
                "max_new_tokens": (  # æœ€å¤§ç”Ÿæˆçš„æ–°tokenæ•°é‡
                    "INT",
                    {"default": 2048, "min": 128, "max": 256000, "step": 1},
                ),
                "min_pixels": (  # å›¾åƒæœ€å°åƒç´ æ•°ï¼ˆä¸æ¨¡å‹å¤„ç†èƒ½åŠ›ç›¸å…³ï¼‰
                    "INT",
                    {
                        "default": 256 * 28 * 28,
                        "min": 4 * 28 * 28,
                        "max": 16384 * 28 * 28,
                        "step": 28 * 28,
                    },
                ),
                "max_pixels": (  # å›¾åƒæœ€å¤§åƒç´ æ•°ï¼ˆä¸æ¨¡å‹å¤„ç†èƒ½åŠ›ç›¸å…³ï¼‰
                    "INT",
                    {
                        "default": 1280 * 28 * 28,
                        "min": 4 * 28 * 28,
                        "max": 16384 * 28 * 28,
                        "step": 28 * 28,
                    },
                ),
                "seed": ("INT", {"default": -1}),  # éšæœºç§å­ï¼ˆ-1è¡¨ç¤ºä¸å›ºå®šï¼‰
                "attention": (  # æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹å¼
                    [
                        "eager",  # æ™®é€šå®ç°
                        "sdpa",  # æ‰©å±•ç‚¹ç§¯æ³¨æ„åŠ›
                        "flash_attention_2",  # Flash Attention 2ï¼ˆæ›´é«˜æ•ˆï¼‰
                    ],
                ),
            },
            "optional": {  # å¯é€‰è¾“å…¥å‚æ•°
                "source_path": ("PATH",),  # å›¾åƒæºè·¯å¾„
                "image": ("IMAGE",),  # è¾“å…¥å›¾åƒï¼ˆä»ComfyUIå…¶ä»–èŠ‚ç‚¹ä¼ å…¥ï¼‰
            },
        }

    RETURN_TYPES = ("STRING",)  # èŠ‚ç‚¹è¿”å›ç±»å‹ï¼šå­—ç¬¦ä¸²ï¼ˆç”Ÿæˆçš„æ–‡æœ¬ï¼‰
    FUNCTION = "inference"  # èŠ‚ç‚¹ä¸»è¦å‡½æ•°å
    CATEGORY = "Comfyui_Qwen3-VL-Instruct"  # èŠ‚ç‚¹åœ¨ComfyUIä¸­çš„åˆ†ç±»

    def inference(
        self,
        text,  # ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        model,  # é€‰æ‹©çš„æ¨¡å‹
        keep_model_loaded,  # æ˜¯å¦ä¿æŒæ¨¡å‹åŠ è½½
        temperature,  # ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
        max_new_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
        min_pixels,  # å›¾åƒæœ€å°åƒç´ æ•°
        max_pixels,  # å›¾åƒæœ€å¤§åƒç´ æ•°
        seed,  # éšæœºç§å­
        quantization,  # é‡åŒ–è®¾ç½®
        source_path=None,  # å›¾åƒæºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        image=None,  # è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
        attention="eager",  # æ³¨æ„åŠ›æœºåˆ¶å®ç°
    ):
        """æ‰§è¡ŒQwen3-VLæ¨¡å‹æ¨ç†"""
        # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æœseedä¸ç­‰äº-1ï¼‰
        if seed != -1:
            torch.manual_seed(seed)
        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹æ„å»ºæ¨¡å‹ID
        if model.startswith("Huihui-"):
            # å¯¹äºHuihuiæ¨¡å‹ï¼Œä½¿ç”¨fireicewolfä½œä¸ºä»“åº“å
            model_id = f"fireicewolf/{model}"
        else:
            # å¯¹äºæ ‡å‡†Qwenæ¨¡å‹ï¼Œä½¿ç”¨qwenä½œä¸ºä»“åº“å
            model_id = f"qwen/{model}"
        
        # æ„å»ºæ¨¡å‹ç›®å½•ï¼ˆç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºç›®å½•åï¼‰
        model_repo_dir = os.path.join(
            folder_paths.models_dir,  # ComfyUIæ¨¡å‹ç›®å½•
            "prompt_generator",  # æç¤ºç”Ÿæˆå™¨æ¨¡å‹å­ç›®å½•
            model  # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºç›®å½•å
        )
        
        self.model_checkpoint = model_repo_dir
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä»ModelScopeä¸‹è½½
        if not os.path.exists(self.model_checkpoint):
            from modelscope import snapshot_download  # å¯¼å…¥ModelScopeä¸‹è½½å·¥å…·
            import traceback  # å¯¼å…¥é”™è¯¯è·Ÿè¸ªæ¨¡å—
            import shutil  # ç”¨äºæ–‡ä»¶æ“ä½œ
            print(f"ğŸš€ å¼€å§‹ä» ModelScope ä¸‹è½½æ¨¡å‹: {model_id}")
            print(f"ğŸ“ ä¸‹è½½åˆ°: {self.model_checkpoint}")
            try:
                # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¸‹è½½
                temp_dir = os.path.join(folder_paths.models_dir, "prompt_generator", ".temp")
                os.makedirs(temp_dir, exist_ok=True)
                
                # ä¸‹è½½æ¨¡å‹åˆ°ä¸´æ—¶ç›®å½•
                snapshot_download(
                    repo_id=model_id,  # æ¨¡å‹ä»“åº“ID
                    cache_dir=temp_dir,  # ä¸´æ—¶ç¼“å­˜ç›®å½•
                )
                
                # å°†ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶ç§»åŠ¨åˆ°æ­£ç¡®çš„ç›®å½•
                downloaded_dir = os.path.join(temp_dir, model_id.replace("/", os.sep))
                if os.path.exists(downloaded_dir):
                    # å¦‚æœç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒ
                    if os.path.exists(self.model_checkpoint):
                        shutil.rmtree(self.model_checkpoint)
                    # ç§»åŠ¨ä¸‹è½½çš„æ¨¡å‹ç›®å½•
                    shutil.move(downloaded_dir, self.model_checkpoint)
                    # åˆ é™¤ä¸´æ—¶ç›®å½•
                    shutil.rmtree(temp_dir)
                
                print(f"âœ… æ¨¡å‹ {model_id} ä¸‹è½½å®Œæˆ!")
            except Exception as e:
                print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}")
                print(f"ğŸ“‹ å®Œæ•´é”™è¯¯ä¿¡æ¯:")
                traceback.print_exc()  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                temp_dir = os.path.join(folder_paths.models_dir, "prompt_generator", ".temp")
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir)
                raise Exception(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_id}ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° {self.model_checkpoint}") from e

        # å¦‚æœæ¨¡å‹IDæˆ–é‡åŒ–è®¾ç½®æ”¹å˜ï¼Œé‡æ–°åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
        if (
            self.current_model_id != model_id  # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¹å˜
            or self.current_quantization != quantization  # æ£€æŸ¥é‡åŒ–è®¾ç½®æ˜¯å¦æ”¹å˜
            or self.processor is None  # æ£€æŸ¥å¤„ç†å™¨æ˜¯å¦æœªåŠ è½½
            or self.model is None  # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœªåŠ è½½
        ):
            # æ›´æ–°å½“å‰æ¨¡å‹ä¿¡æ¯
            self.current_model_id = model_id
            self.current_quantization = quantization
            
            # é‡Šæ”¾ä¹‹å‰åŠ è½½çš„èµ„æº
            if self.processor is not None:
                del self.processor  # åˆ é™¤å¤„ç†å™¨
                self.processor = None
            if self.model is not None:
                del self.model  # åˆ é™¤æ¨¡å‹
                self.model = None
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
                torch.cuda.ipc_collect()  # æ”¶é›†åƒåœ¾
            # åŠ è½½æ¨¡å‹å¤„ç†å™¨ï¼ˆç”¨äºæ–‡æœ¬å’Œå›¾åƒé¢„å¤„ç†ï¼‰
            self.processor = AutoProcessor.from_pretrained(
                self.model_checkpoint,  # æ¨¡å‹è·¯å¾„
                min_pixels=min_pixels,  # å›¾åƒæœ€å°åƒç´ æ•°
                max_pixels=max_pixels,  # å›¾åƒæœ€å¤§åƒç´ æ•°
            )
            # æ ¹æ®é€‰æ‹©çš„é‡åŒ–è®¾ç½®åˆ›å»ºé‡åŒ–é…ç½®
            if quantization == "4bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,  # å¯ç”¨4ä½é‡åŒ–
                )
            elif quantization == "8bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,  # å¯ç”¨8ä½é‡åŒ–
                )
            else:
                quantization_config = None  # ä¸ä½¿ç”¨é‡åŒ–

            # åŠ è½½Qwen3-VLæ¨¡å‹
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                self.model_checkpoint,  # æ¨¡å‹è·¯å¾„
                dtype=torch.bfloat16 if self.bf16_support else torch.float16,  # æ•°æ®ç±»å‹ï¼ˆä¼˜å…ˆä½¿ç”¨bfloat16ï¼‰
                device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆCPU/GPUï¼‰
                attn_implementation=attention,  # æ³¨æ„åŠ›æœºåˆ¶å®ç°
                quantization_config=quantization_config,  # é‡åŒ–é…ç½®
            )

        # å¤„ç†è¾“å…¥å›¾åƒ
        temp_path = None
        if image is not None:
            # å°†ComfyUIçš„å›¾åƒå¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
            pil_image = ToPILImage()(image[0].permute(2, 0, 1))
            # åˆ›å»ºä¸´æ—¶å›¾åƒæ–‡ä»¶è·¯å¾„
            temp_path = Path(folder_paths.temp_directory) / f"temp_image_{seed}.png"
            # ä¿å­˜ä¸´æ—¶å›¾åƒæ–‡ä»¶
            pil_image.save(temp_path)

        # å¼€å§‹æ¨ç†ï¼ˆtorch.no_grad()è¡¨ç¤ºä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜ï¼‰
        with torch.no_grad():
            # æ ¹æ®è¾“å…¥ç±»å‹æ„å»ºæ¶ˆæ¯æ ¼å¼
            if source_path:
                # å¦‚æœæä¾›äº†å›¾åƒæºè·¯å¾„
                messages = [
                    {
                        "role": "system",
                        "content": "You are QwenVL, you are a helpful assistant expert in turning images into words.",
                    },
                    {
                        "role": "user",
                        "content": source_path
                        + [
                            {"type": "text", "text": text},
                        ],
                    },
                ]
            elif temp_path:
                # å¦‚æœæä¾›äº†ä¸´æ—¶å›¾åƒæ–‡ä»¶
                messages = [
                    {
                        "role": "system",
                        "content": "You are QwenVL, you are a helpful assistant expert in turning images into words.",
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "image": f"file://{temp_path}"},  # å›¾åƒè·¯å¾„
                            {"type": "text", "text": text},  # ç”¨æˆ·è¾“å…¥æ–‡æœ¬
                        ],
                    },
                ]
            else:
                # åªæœ‰æ–‡æœ¬è¾“å…¥
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": text},
                        ],
                    }
                ]

            # æ¨ç†å‰å‡†å¤‡
            # åº”ç”¨èŠå¤©æ¨¡æ¿æ ¼å¼åŒ–æ–‡æœ¬
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            # å¤„ç†æ¶ˆæ¯ä¸­çš„è§†è§‰ä¿¡æ¯
            image_inputs, video_inputs = process_vision_info(messages)
            # é¢„å¤„ç†è¾“å…¥ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰
            inputs = self.processor(
                text=[text],  # è¾“å…¥æ–‡æœ¬
                images=image_inputs,  # è¾“å…¥å›¾åƒ
                videos=video_inputs,  # è¾“å…¥è§†é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
                padding=True,  # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                return_tensors="pt",  # è¿”å›PyTorchå¼ é‡
            )
            # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPUï¼‰
            inputs = inputs.to(self.device)
            # æ‰§è¡Œæ¨ç†ï¼šç”Ÿæˆè¾“å‡º
            generated_ids = self.model.generate(
                **inputs,  # è¾“å…¥æ•°æ®
                max_new_tokens=max_new_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                temperature=temperature,  # éšæœºæ€§å‚æ•°
            )
            # è£å‰ªç”Ÿæˆçš„tokenï¼ˆåªä¿ç•™æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids_trimmed = [
                out_ids[len(in_ids) :]  # ä»è¾“å…¥é•¿åº¦ä¹‹åå¼€å§‹æˆªå–
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            # å°†ç”Ÿæˆçš„tokenè§£ç ä¸ºæ–‡æœ¬
            result = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,  # è·³è¿‡ç‰¹æ®Štoken
                clean_up_tokenization_spaces=False,  # ä¸æ¸…ç†ç©ºæ ¼
                temperature=temperature,
            )

            # å¦‚æœä¸éœ€è¦ä¿æŒæ¨¡å‹åŠ è½½ï¼Œé‡Šæ”¾èµ„æº
            if not keep_model_loaded:
                del self.processor  # é‡Šæ”¾å¤„ç†å™¨å†…å­˜
                del self.model  # é‡Šæ”¾æ¨¡å‹å†…å­˜
                self.processor = None  # å°†å¤„ç†å™¨è®¾ç½®ä¸ºNone
                self.model = None  # å°†æ¨¡å‹è®¾ç½®ä¸ºNone
                self.current_model_id = None  # é‡ç½®å½“å‰æ¨¡å‹ID
                self.current_quantization = None  # é‡ç½®å½“å‰é‡åŒ–è®¾ç½®
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
                    torch.cuda.ipc_collect()  # æ”¶é›†åƒåœ¾

            # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬ç»“æœ
            return (result,)
